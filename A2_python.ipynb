{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import interpolate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label names    \n",
    "label_names = loadmat('label_names.mat', squeeze_me=True)\n",
    "activity_names_indexed = label_names['activity_names_indexed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract raw data from files stored in allData folder\n",
    "def compute_raw_data(dir_name):\n",
    "\n",
    "    # Load label names    \n",
    "    label_names = loadmat('label_names.mat', squeeze_me=True)\n",
    "    activity_names_indexed = label_names['activity_names_indexed']\n",
    "\n",
    "    # Function to enumerate and assign number labels against different activity names\n",
    "    string_to_number = {string: number for number, string in enumerate(activity_names_indexed, start=1)}\n",
    "\n",
    "    # Initialize data arrays\n",
    "    y_accel_all = []\n",
    "    y_bar_all = []\n",
    "    y_bar_ts_all = []\n",
    "    y_label_all = []\n",
    "    y_user_all = []\n",
    "    y_activity_all = []\n",
    "    column_names_accel = ['ts', 'accel_x', 'accel_y','accel_z']\n",
    "    y_accel_all_df = pd.DataFrame(columns= column_names_accel)\n",
    "\n",
    "    # Iterate through accel files\n",
    "    files = [f for f in os.listdir(dir_name) if f.endswith('-accel.txt')]\n",
    "    for file_name in files:\n",
    "        # Extracting activity name, netID from file names\n",
    "        # for files where there is no IMEI number\n",
    "        if(file_name[0:4]=='null'):\n",
    "            substr = dir_name[dir_name.find(\"allData/\")+8:]\n",
    "            file_name_prefix = file_name[:-(11+len(substr))]\n",
    "            file_name_common = file_name[:-(10)]\n",
    "            activity_name = file_name[25:-(11+len(substr))]\n",
    "        # for files where there is a 15 digit IMEI number  \n",
    "        else:\n",
    "            substr = dir_name[dir_name.find(\"allData/\")+8:]\n",
    "            file_name_prefix = file_name[:-(11+len(substr))]\n",
    "            file_name_common = file_name[:-(10)]\n",
    "            activity_name = file_name[36:-(11+len(substr))]\n",
    "\n",
    "        for activity_index in range(1, len(activity_names_indexed) + 1):\n",
    "            if len([1 for name in activity_names_indexed if activity_name in name]) > 0:\n",
    "                break\n",
    "\n",
    "        # loading acceleration data    \n",
    "        accel_data = np.loadtxt(os.path.join(dir_name, file_name), delimiter=',')\n",
    "        ts, accel_x, accel_y, accel_z = accel_data[:, 0], accel_data[:, 1], accel_data[:, 2], accel_data[:, 3]\n",
    "\n",
    "\n",
    "        # Removing duplicate timestamps\n",
    "        ts_same = np.where(ts[:-1] == ts[1:])[0]\n",
    "        accel_data = np.delete(accel_data, ts_same, axis=0)\n",
    "\n",
    "        # storing data in dataframe\n",
    "        column_names_accel = ['ts', 'accel_x', 'accel_y','accel_z']\n",
    "        df_accel_data = pd.DataFrame(accel_data,columns= column_names_accel)\n",
    "\n",
    "        # sorting data based on timestamp and interpolating data \n",
    "        df_accel_data = df_accel_data.sort_values('ts')\n",
    "        df_accel_data = df_accel_data.interpolate(method='spline', order=2)\n",
    "\n",
    "        # time to exclude (first and last few seconds are excluded from analysis)\n",
    "        time_to_exclude = 2\n",
    "        df_accel_data_trim = df_accel_data.iloc[time_to_exclude*32:-1*time_to_exclude*32]\n",
    "\n",
    "        # for the same activity and netID, extracting the pressure values as well\n",
    "\n",
    "        file_name_baro = file_name_common+\"-pressure.txt\"\n",
    "        bar_data = np.loadtxt(os.path.join(dir_name, file_name_baro), delimiter=',')\n",
    "        ts_bar, y_bar = bar_data[:, 0], bar_data[:, 1]\n",
    "\n",
    "        # Remove duplicate timestamps\n",
    "        ts_same_bar = np.where(ts_bar[:-1] == ts_bar[1:])[0]\n",
    "        bar_data = np.delete(bar_data, ts_same_bar, axis=0)\n",
    "\n",
    "        # checking acceleration and pressure data length\n",
    "        desired_length = len(df_accel_data)\n",
    "        bar_data_len = len(bar_data)\n",
    "\n",
    "        # Interpolate barometer data to match the length of accel_data\n",
    "        interp_indices = np.linspace(0, bar_data_len - 1, desired_length)\n",
    "        y_bar_interp = interp1d(np.arange(bar_data_len), bar_data[:, 1], kind='linear', fill_value='extrapolate')\n",
    "        y_bar_interpolated = y_bar_interp(interp_indices)\n",
    "\n",
    "        # Smooth interpolated barometer data\n",
    "        window_size = 4 * 128\n",
    "        y_bar_smoothed = savgol_filter(y_bar_interpolated, window_size, 1,mode='nearest')\n",
    "\n",
    "        # Trim data based on time_to_exclude\n",
    "        start_idx = int(time_to_exclude * 32)\n",
    "        end_idx = -start_idx if start_idx > 0 else None\n",
    "        y_bar_trimmed = y_bar_smoothed[start_idx:end_idx]\n",
    "        \n",
    "        # if activity is present in list of activities\n",
    "        if activity_name in activity_names_indexed:\n",
    "\n",
    "            y_label = np.full(len(y_bar_trimmed), string_to_number[activity_name])\n",
    "            y_activity = np.full(len(y_bar_trimmed), activity_name)\n",
    "            y_user = np.full(len(y_bar_trimmed), substr)\n",
    "\n",
    "            # only sets of 128 data is processed\n",
    "            multiple_of_128 = 128 * (len(y_label) // 128)\n",
    "            y_bar_f = y_bar_trimmed[:multiple_of_128]\n",
    "            y_label_f = y_label[:multiple_of_128]\n",
    "            y_user_f = y_user[:multiple_of_128]\n",
    "            y_activity_f = y_activity[:multiple_of_128]\n",
    "\n",
    "            y_accel_f = df_accel_data_trim.iloc[:multiple_of_128, :]\n",
    "\n",
    "            # Concatenate results into new arrays\n",
    "            y_accel_all_df = pd.concat([y_accel_all_df, y_accel_f],ignore_index=True)\n",
    "            y_label_all = np.concatenate((y_label_all, y_label_f))\n",
    "            y_user_all = np.concatenate((y_user_all, y_user_f))\n",
    "            y_activity_all = np.concatenate((y_activity_all, y_activity_f))\n",
    "            y_bar_all = np.concatenate((y_bar_all, y_bar_f))\n",
    "\n",
    "    # add pressure,label,user and activity values into dataframe  \n",
    "    y_accel_all_df['y_bar'] = y_bar_all\n",
    "    y_accel_all_df['label'] = y_label_all\n",
    "    y_accel_all_df['user'] = y_user_all\n",
    "    y_accel_all_df['activity'] = y_activity_all\n",
    "\n",
    "    # returning final dataframe\n",
    "    return y_accel_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO \n",
    "\n",
    "Set your NetID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your netid\n",
    "my_netid = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of data directory\n",
    "data_dir = os.path.join(os.getcwd(), 'allData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_raw = ['ts', 'accel_x', 'accel_y','accel_z','y_bar','label','user','activity']\n",
    "df_raw = pd.DataFrame(columns= col_names_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all raw data by iterating through the files\n",
    "\n",
    "for data_dir_name in os.listdir(data_dir):\n",
    "    # Goes through all of the directories representing all imei addresses\n",
    "    if os.path.isdir(os.path.join(data_dir, data_dir_name)) and data_dir_name[0] != '.':\n",
    "\n",
    "        print(f'Processing directory {data_dir_name}')\n",
    "        # Compute raw data\n",
    "        dir_path = os.path.join(data_dir, data_dir_name)\n",
    "        df_raw_temp = compute_raw_data(dir_path)\n",
    "\n",
    "        df_raw_temp = df_raw_temp.sort_values(by = ['label', 'ts'], ignore_index=True)\n",
    "        df_raw = pd.concat([df_raw, df_raw_temp],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of activities \n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize = (20, 5))\n",
    "sns.countplot(x = \"activity\", data = df_raw)\n",
    "plt.title(\"Number of samples by activity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of activities among different users\n",
    "\n",
    "plt.figure(figsize = (18, 6))\n",
    "sns.countplot(x = \"user\", hue = \"activity\", data = df_raw)\n",
    "plt.title(\"Activities by Users\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train data and test data \n",
    "df_train = df_raw[df_raw['user'] != my_netid]\n",
    "df_test = df_raw[df_raw['user'] == my_netid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of raw data\n",
    "# Note how the magnitudes and signal varies for each activity\n",
    "\n",
    "for i in [\"Stationary\", \"Running\", \"Walking-flat-surface\"]:\n",
    "    data_x = df_raw[(df_raw[\"user\"] == my_netid) & (df_raw[\"activity\"] == i)][:1000]\n",
    "    plt.figure(figsize = (15, 6))\n",
    "    sns.lineplot(y = \"accel_x\", x = \"ts\", data = data_x)\n",
    "    sns.lineplot(y = \"accel_y\", x = \"ts\", data = data_x)\n",
    "    sns.lineplot(y = \"accel_z\", x = \"ts\", data = data_x)\n",
    "    plt.legend([\"accel_x\", \"accel_y\", \"accel_z\"])\n",
    "    plt.ylabel(i)\n",
    "    plt.title(i, fontsize = 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    " \n",
    "Do you see any interesting trends while observing signals from different activities (like walking, running stationary) ? Explain what you see. \n",
    "Can you think of any specific feature that might help us to differentiate among different activities? You will write a paragraph on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list_train = []\n",
    "y_list_train = []\n",
    "z_list_train = []\n",
    "b_list_train = []\n",
    "train_labels = []\n",
    "\n",
    "# TO DO\n",
    "# Similarly create test list\n",
    "\n",
    "# Setting window size of 100 datapoints with an overlap of 50%\n",
    "window_size = 100\n",
    "step_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "You will extract features from the accelerometer magnitude and barometric pressure time series data. The windowing will be done by a sliding window where the length of the window will be 100 data points with an overlap of 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating overlaping windows of size 100\n",
    "for i in range(0, df_train.shape[0] - window_size, step_size):\n",
    "    xs = df_raw['accel_x'].values[i: i + 100]\n",
    "\n",
    "\n",
    "    x_list_train.append(xs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "Compute Different Time Domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Time Domain Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "Compute Different Frequency Domain Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there are NaN values in features extracted, extrapolate from neighbouring values.(Like mean of above and below value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Frequency domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While computing PSD features, you can use the below sampling rate. You can use signal.welch function to compute the features.\n",
    "import scipy.signal as signal\n",
    "sample_rate=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "Prepare a TSNE plot from the extracted features.See if the plot can differentiate between stationary and moving activity labels. Briefly comment on the same. \n",
    "\n",
    "You can use 'from sklearn.manifold import TSNE' package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "KNN and Random Forest Classification.\n",
    "Compute the confusion matrix and then compute the precision, recall and F1 score for each activity separately.\n",
    "\n",
    "compare the performance of these two classifiers and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "Time Domain vs Frequency Domain\n",
    "\n",
    "Use just time domain and frequency domain features seperately and run the same classifier. Which domain ( frequency domain vs time domain ) is helping you the most in terms of building your activity recognition system ? Comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD vs FD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "Cross validation- you will implement Leave One Subject Out Cross Validation . \n",
    "\n",
    "For this task, you are free to select any classifier you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS POINTS\n",
    "\n",
    "Try building a basic CNN model and use either the raw time series data or the extracted feature set as input and try to classify the different activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
